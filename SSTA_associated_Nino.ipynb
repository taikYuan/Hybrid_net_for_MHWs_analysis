{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83015aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b08687",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    A set of functions which implement the Marine Heat Wave (MHW)\n",
    "    definition of Hobday et al. (2016)\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "from scipy import stats\n",
    "import scipy.ndimage as ndimage\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def detect(t, temp, climatologyPeriod=[None,None], pctile=90, windowHalfWidth=5, smoothPercentile=True, smoothPercentileWidth=31, minDuration=5, joinAcrossGaps=True, maxGap=2, maxPadLength=False, coldSpells=False, alternateClimatology=False, Ly=False):\n",
    "    '''\n",
    "    Applies the Hobday et al. (2016) marine heat wave definition to an input time\n",
    "    series of temp ('temp') along with a time vector ('t'). Outputs properties of\n",
    "    all detected marine heat waves.\n",
    "    Inputs:\n",
    "      t       Time vector, in datetime format (e.g., date(1982,1,1).toordinal())\n",
    "              [1D numpy array of length T]\n",
    "      temp    Temperature vector [1D numpy array of length T]\n",
    "    Outputs:\n",
    "      mhw     Detected marine heat waves (MHWs). Each key (following list) is a\n",
    "              list of length N where N is the number of detected MHWs:\n",
    " \n",
    "        'time_start'           Start time of MHW [datetime format] # MHW开始的时间\n",
    "        'time_end'             End time of MHW [datetime format]  # MHW结束的时间\n",
    "        'time_peak'            Time of MHW peak [datetime format] # MHW的峰值时间\n",
    "        'date_start'           Start date of MHW [datetime format]  # MHW开始的日期\n",
    "        'date_end'             End date of MHW [datetime format]   # MHW结束的日期\n",
    "        'date_peak'            Date of MHW peak [datetime format] # MHW峰值的时间\n",
    "        'index_start'          Start index of MHW #MHW开始的索引\n",
    "        'index_end'            End index of MHW  #MHW结束的索引\n",
    "        'index_peak'           Index of MHW peak #MHW峰值的索引\n",
    "        'duration'             Duration of MHW [days] #MHW经历的日期\n",
    "        'intensity_max'        Maximum (peak) intensity [deg. C]  # 最大强度\n",
    "        'intensity_mean'       Mean intensity [deg. C] #平均强度\n",
    "        'intensity_var'        Intensity variability [deg. C] # \n",
    "        'intensity_cumulative' Cumulative intensity [deg. C x days] #累积强度\n",
    "        'rate_onset'           Onset rate of MHW [deg. C / days] # MHW的增长率\n",
    "        'rate_decline'         Decline rate of MHW [deg. C / days]\n",
    "        'intensity_max_relThresh', 'intensity_mean_relThresh', 'intensity_var_relThresh', \n",
    "        and 'intensity_cumulative_relThresh' are as above except relative to the\n",
    "        threshold (e.g., 90th percentile) rather than the seasonal climatology\n",
    "        'intensity_max_abs', 'intensity_mean_abs', 'intensity_var_abs', and\n",
    "        'intensity_cumulative_abs' are as above except as absolute magnitudes\n",
    "        rather than relative to the seasonal climatology or threshold\n",
    "        'category' is an integer category system (1, 2, 3, 4) based on the maximum intensity\n",
    "        in multiples of threshold exceedances, i.e., a value of 1 indicates the MHW\n",
    "        intensity (relative to the climatology) was >=1 times the value of the threshold (but\n",
    "        less than 2 times; relative to climatology, i.e., threshold - climatology).\n",
    "        Category types are defined as 1=moderate, 2=strong, 3=severe, 4=extreme. More details in\n",
    "        Hobday et al. (in prep., Oceanography). Also supplied are the duration of each of these\n",
    "        categories for each event.  #这些相对于阈值而不是相对于气候态\n",
    "        'n_events'             A scalar integer (not a list) indicating the total\n",
    "                               number of detected MHW events  # 发生了几次MHW事件\n",
    "      clim    Climatology of SST. Each key (following list) is a seasonally-varying\n",
    "              time series [1D numpy array of length T] of a particular measure:  # 海温的气候学\n",
    "        'thresh'               Seasonally varying threshold (e.g., 90th percentile) # 季节性变化的阈值 （例如 90百分位数）\n",
    "        'seas'                 Climatological seasonal cycle  # 气候季节循环\n",
    "        'missing'              A vector of TRUE/FALSE indicating which elements in \n",
    "                               temp were missing values for the MHWs detection\n",
    "    Options:\n",
    "      climatologyPeriod      Period over which climatology is calculated, specified\n",
    "                             as list of start and end years. Default is to calculate\n",
    "                             over the full range of years in the supplied time series.\n",
    "                             Alternate periods suppled as a list e.g. [1983,2012].\n",
    "      pctile                 Threshold percentile (%) for detection of extreme values\n",
    "                             (DEFAULT = 90)\n",
    "      windowHalfWidth        Width of window (one sided) about day-of-year used for\n",
    "                             the pooling of values and calculation of threshold percentile\n",
    "                             (DEFAULT = 5 [days])\n",
    "      smoothPercentile       Boolean switch indicating whether to smooth the threshold\n",
    "                             percentile timeseries with a moving average (DEFAULT = True)\n",
    "      smoothPercentileWidth  Width of moving average window for smoothing threshold\n",
    "                             (DEFAULT = 31 [days])\n",
    "      minDuration            Minimum duration for acceptance detected MHWs\n",
    "                             (DEFAULT = 5 [days])\n",
    "      joinAcrossGaps         Boolean switch indicating whether to join MHWs\n",
    "                             which occur before/after a short gap (DEFAULT = True)\n",
    "      maxGap                 Maximum length of gap allowed for the joining of MHWs\n",
    "                             (DEFAULT = 2 [days])\n",
    "      maxPadLength           Specifies the maximum length [days] over which to interpolate\n",
    "                             (pad) missing data (specified as nans) in input temp time series.\n",
    "                             i.e., any consecutive blocks of NaNs with length greater\n",
    "                             than maxPadLength will be left as NaN. Set as an integer.\n",
    "                             (DEFAULT = False, interpolates over all missing values).\n",
    "      coldSpells             Specifies if the code should detect cold events instead of\n",
    "                             heat events. (DEFAULT = False)\n",
    "      alternateClimatology   Specifies an alternate temperature time series to use for the\n",
    "                             calculation of the climatology. Format is as a list of numpy\n",
    "                             arrays: (1) the first element of the list is a time vector,\n",
    "                             in datetime format (e.g., date(1982,1,1).toordinal())\n",
    "                             [1D numpy array of length TClim] and (2) the second element of\n",
    "                             the list is a temperature vector [1D numpy array of length TClim].\n",
    "                             (DEFAULT = False)\n",
    "      Ly                     Specifies if the length of the year is < 365/366 days (e.g. a \n",
    "                             360 day year from a climate model). This affects the calculation\n",
    "                             of the climatology. (DEFAULT = False)\n",
    "    Notes:\n",
    "      1. This function assumes that the input time series consist of continuous daily values\n",
    "         with few missing values. Time ranges which start and end part-way through the calendar\n",
    "         year are supported.\n",
    "      2. This function supports leap years. This is done by ignoring Feb 29s for the initial\n",
    "         calculation of the climatology and threshold. The value of these for Feb 29 is then\n",
    "         linearly interpolated from the values for Feb 28 and Mar 1.\n",
    "      3. The calculation of onset and decline rates assumes that the heat wave started a half-day\n",
    "         before the start day and ended a half-day after the end-day. (This is consistent with the\n",
    "         duration definition as implemented, which assumes duration = end day - start day + 1.)\n",
    "      4. For the purposes of MHW detection, any missing temp values not interpolated over (through\n",
    "         optional maxPadLLength) will be set equal to the seasonal climatology. This means they will\n",
    "         trigger the end/start of any adjacent temp values which satisfy the MHW criteria.\n",
    "      5. If the code is used to detect cold events (coldSpells = True), then it works just as for heat\n",
    "         waves except that events are detected as deviations below the (100 - pctile)th percentile\n",
    "         (e.g., the 10th instead of 90th) for at least 5 days. Intensities are reported as negative\n",
    "         values and represent the temperature anomaly below climatology.\n",
    "    Written by Eric Oliver, Institue for Marine and Antarctic Studies, University of Tasmania, Feb 2015\n",
    "    '''\n",
    "\n",
    "    #\n",
    "    # Initialize MHW output variable\n",
    "    #\n",
    "\n",
    "    mhw = {}\n",
    "    mhw['time_start'] = [] # datetime format\n",
    "    mhw['time_end'] = [] # datetime format\n",
    "    mhw['time_peak'] = [] # datetime format\n",
    "    mhw['date_start'] = [] # datetime format\n",
    "    mhw['date_end'] = [] # datetime format\n",
    "    mhw['date_peak'] = [] # datetime format\n",
    "    mhw['index_start'] = []\n",
    "    mhw['index_end'] = []\n",
    "    mhw['index_peak'] = []\n",
    "    mhw['duration'] = [] # [days]\n",
    "    mhw['duration_moderate'] = [] # [days]\n",
    "    mhw['duration_strong'] = [] # [days]\n",
    "    mhw['duration_severe'] = [] # [days]\n",
    "    mhw['duration_extreme'] = [] # [days]\n",
    "    mhw['intensity_max'] = [] # [deg C]\n",
    "    mhw['intensity_mean'] = [] # [deg C]\n",
    "    mhw['intensity_var'] = [] # [deg C]\n",
    "    mhw['intensity_cumulative'] = [] # [deg C]\n",
    "    mhw['intensity_max_relThresh'] = [] # [deg C]\n",
    "    mhw['intensity_mean_relThresh'] = [] # [deg C]\n",
    "    mhw['intensity_var_relThresh'] = [] # [deg C]\n",
    "    mhw['intensity_cumulative_relThresh'] = [] # [deg C]\n",
    "    mhw['intensity_max_abs'] = [] # [deg C]\n",
    "    mhw['intensity_mean_abs'] = [] # [deg C]\n",
    "    mhw['intensity_var_abs'] = [] # [deg C]\n",
    "    mhw['intensity_cumulative_abs'] = [] # [deg C]\n",
    "    mhw['category'] = []\n",
    "    mhw['rate_onset'] = [] # [deg C / day]\n",
    "    mhw['rate_decline'] = [] # [deg C / day]\n",
    "\n",
    "    #\n",
    "    # Time and dates vectors\n",
    "    #\n",
    "\n",
    "    # Generate vectors for year, month, day-of-month, and day-of-year\n",
    "    T = len(t)\n",
    "    year = np.zeros((T))\n",
    "    month = np.zeros((T))\n",
    "    day = np.zeros((T))\n",
    "    doy = np.zeros((T))\n",
    "    for i in range(T):\n",
    "        year[i] = date.fromordinal(t[i]).year\n",
    "        month[i] = date.fromordinal(t[i]).month\n",
    "        day[i] = date.fromordinal(t[i]).day\n",
    "    # Leap-year baseline for defining day-of-year values\n",
    "    year_leapYear = 2012 # This year was a leap-year and therefore doy in range of 1 to 366\n",
    "    t_leapYear = np.arange(date(year_leapYear, 1, 1).toordinal(),date(year_leapYear, 12, 31).toordinal()+1)\n",
    "    dates_leapYear = [date.fromordinal(tt.astype(int)) for tt in t_leapYear]\n",
    "    month_leapYear = np.zeros((len(t_leapYear)))\n",
    "    day_leapYear = np.zeros((len(t_leapYear)))\n",
    "    doy_leapYear = np.zeros((len(t_leapYear)))\n",
    "    for tt in range(len(t_leapYear)):\n",
    "        month_leapYear[tt] = date.fromordinal(t_leapYear[tt]).month\n",
    "        day_leapYear[tt] = date.fromordinal(t_leapYear[tt]).day\n",
    "        doy_leapYear[tt] = t_leapYear[tt] - date(date.fromordinal(t_leapYear[tt]).year,1,1).toordinal() + 1\n",
    "        \n",
    "    # Calculate day-of-year values\n",
    "    for tt in range(T):\n",
    "        doy[tt] = doy_leapYear[(month_leapYear == month[tt]) * (day_leapYear == day[tt])]\n",
    "\n",
    "    # Constants (doy values for Feb-28 and Feb-29) for handling leap-years\n",
    "    feb28 = 59\n",
    "    feb29 = 60\n",
    "\n",
    "    # Set climatology period, if unset use full range of available data\n",
    "    if (climatologyPeriod[0] is None) or (climatologyPeriod[1] is None):\n",
    "        climatologyPeriod[0] = year[0]\n",
    "        climatologyPeriod[1] = year[-1]\n",
    "\n",
    "    #\n",
    "    # Calculate threshold and seasonal climatology (varying with day-of-year)\n",
    "    #\n",
    "\n",
    "    # if alternate temperature time series is supplied for the calculation of the climatology\n",
    "    if alternateClimatology:\n",
    "        tClim = alternateClimatology[0]\n",
    "        tempClim = alternateClimatology[1]\n",
    "        TClim = len(tClim)\n",
    "        yearClim = np.zeros((TClim))\n",
    "        monthClim = np.zeros((TClim))\n",
    "        dayClim = np.zeros((TClim))\n",
    "        doyClim = np.zeros((TClim))\n",
    "        for i in range(TClim):\n",
    "            yearClim[i] = date.fromordinal(tClim[i]).year\n",
    "            monthClim[i] = date.fromordinal(tClim[i]).month\n",
    "            dayClim[i] = date.fromordinal(tClim[i]).day\n",
    "            doyClim[i] = doy_leapYear[(month_leapYear == monthClim[i]) * (day_leapYear == dayClim[i])]\n",
    "    else:\n",
    "        tempClim = temp.copy()\n",
    "        TClim = np.array([T]).copy()[0]\n",
    "        yearClim = year.copy()\n",
    "        monthClim = month.copy()\n",
    "        dayClim = day.copy()\n",
    "        doyClim = doy.copy()\n",
    "\n",
    "    # Flip temp time series if detecting cold spells\n",
    "    if coldSpells:\n",
    "        temp = -1.*temp\n",
    "        tempClim = -1.*tempClim\n",
    "\n",
    "    # Pad missing values for all consecutive missing blocks of length <= maxPadLength\n",
    "    if maxPadLength:\n",
    "        temp = pad(temp, maxPadLength=maxPadLength)\n",
    "        tempClim = pad(tempClim, maxPadLength=maxPadLength)\n",
    "\n",
    "    # Length of climatological year\n",
    "    lenClimYear = 366\n",
    "    # Start and end indices\n",
    "    clim_start = np.where(yearClim == climatologyPeriod[0])[0][0]\n",
    "    clim_end = np.where(yearClim == climatologyPeriod[1])[0][-1]\n",
    "    # Inialize arrays\n",
    "    thresh_climYear = np.NaN*np.zeros(lenClimYear)\n",
    "    seas_climYear = np.NaN*np.zeros(lenClimYear)\n",
    "    clim = {}\n",
    "    clim['thresh'] = np.NaN*np.zeros(TClim)\n",
    "    clim['seas'] = np.NaN*np.zeros(TClim)\n",
    "    # Loop over all day-of-year values, and calculate threshold and seasonal climatology across years\n",
    "    for d in range(1,lenClimYear+1):\n",
    "        # Special case for Feb 29\n",
    "        if d == feb29:\n",
    "            continue\n",
    "        # find all indices for each day of the year +/- windowHalfWidth and from them calculate the threshold\n",
    "        tt0 = np.where(doyClim[clim_start:clim_end+1] == d)[0] \n",
    "        # If this doy value does not exist (i.e. in 360-day calendars) then skip it\n",
    "        if len(tt0) == 0:\n",
    "            continue\n",
    "        tt = np.array([])\n",
    "        for w in range(-windowHalfWidth, windowHalfWidth+1):\n",
    "            tt = np.append(tt, clim_start+tt0 + w)\n",
    "        tt = tt[tt>=0] # Reject indices \"before\" the first element\n",
    "        tt = tt[tt<TClim] # Reject indices \"after\" the last element\n",
    "        thresh_climYear[d-1] = np.nanpercentile(tempClim[tt.astype(int)], pctile)\n",
    "        seas_climYear[d-1] = np.nanmean(tempClim[tt.astype(int)])\n",
    "    # Special case for Feb 29\n",
    "    thresh_climYear[feb29-1] = 0.5*thresh_climYear[feb29-2] + 0.5*thresh_climYear[feb29]\n",
    "    seas_climYear[feb29-1] = 0.5*seas_climYear[feb29-2] + 0.5*seas_climYear[feb29]\n",
    "\n",
    "    # Smooth if desired\n",
    "    if smoothPercentile:\n",
    "        # If the length of year is < 365/366 (e.g. a 360 day year from a Climate Model)\n",
    "        if Ly:\n",
    "            valid = ~np.isnan(thresh_climYear)\n",
    "            thresh_climYear[valid] = runavg(thresh_climYear[valid], smoothPercentileWidth)\n",
    "            valid = ~np.isnan(seas_climYear)\n",
    "            seas_climYear[valid] = runavg(seas_climYear[valid], smoothPercentileWidth)\n",
    "        # >= 365-day year\n",
    "        else:\n",
    "            thresh_climYear = runavg(thresh_climYear, smoothPercentileWidth)\n",
    "            seas_climYear = runavg(seas_climYear, smoothPercentileWidth)\n",
    "\n",
    "    # Generate threshold for full time series\n",
    "    clim['thresh'] = thresh_climYear[doy.astype(int)-1]\n",
    "    clim['seas'] = seas_climYear[doy.astype(int)-1]\n",
    "\n",
    "    # Save vector indicating which points in temp are missing values\n",
    "    clim['missing'] = np.isnan(temp)\n",
    "    # Set all remaining missing temp values equal to the climatology\n",
    "    temp[np.isnan(temp)] = clim['seas'][np.isnan(temp)]\n",
    "\n",
    "    #\n",
    "    # Find MHWs as exceedances above the threshold\n",
    "    #\n",
    "\n",
    "    # Time series of \"True\" when threshold is exceeded, \"False\" otherwise\n",
    "    exceed_bool = temp - clim['thresh']\n",
    "    exceed_bool[exceed_bool<=0] = False\n",
    "    exceed_bool[exceed_bool>0] = True\n",
    "    # Fix issue where missing temp vaues (nan) are counted as True\n",
    "    exceed_bool[np.isnan(exceed_bool)] = False\n",
    "    # Find contiguous regions of exceed_bool = True\n",
    "    events, n_events = ndimage.label(exceed_bool)\n",
    "\n",
    "    # Find all MHW events of duration >= minDuration\n",
    "    for ev in range(1,n_events+1):\n",
    "        event_duration = (events == ev).sum()\n",
    "        if event_duration < minDuration:\n",
    "            continue\n",
    "        mhw['time_start'].append(t[np.where(events == ev)[0][0]])\n",
    "        mhw['time_end'].append(t[np.where(events == ev)[0][-1]])\n",
    "\n",
    "    # Link heat waves that occur before and after a short gap (gap must be no longer than maxGap)\n",
    "    if joinAcrossGaps:\n",
    "        # Calculate gap length for each consecutive pair of events\n",
    "        gaps = np.array(mhw['time_start'][1:]) - np.array(mhw['time_end'][0:-1]) - 1\n",
    "        if len(gaps) > 0:\n",
    "            while gaps.min() <= maxGap:\n",
    "                # Find first short gap\n",
    "                ev = np.where(gaps <= maxGap)[0][0]\n",
    "                # Extend first MHW to encompass second MHW (including gap)\n",
    "                mhw['time_end'][ev] = mhw['time_end'][ev+1]\n",
    "                # Remove second event from record\n",
    "                del mhw['time_start'][ev+1]\n",
    "                del mhw['time_end'][ev+1]\n",
    "                # Calculate gap length for each consecutive pair of events\n",
    "                gaps = np.array(mhw['time_start'][1:]) - np.array(mhw['time_end'][0:-1]) - 1\n",
    "                if len(gaps) == 0:\n",
    "                    break\n",
    "\n",
    "    # Calculate marine heat wave properties\n",
    "    mhw['n_events'] = len(mhw['time_start'])\n",
    "    categories = np.array(['Moderate', 'Strong', 'Severe', 'Extreme'])\n",
    "    for ev in range(mhw['n_events']):\n",
    "        mhw['date_start'].append(date.fromordinal(mhw['time_start'][ev]))\n",
    "        mhw['date_end'].append(date.fromordinal(mhw['time_end'][ev]))\n",
    "        # Get SST series during MHW event, relative to both threshold and to seasonal climatology\n",
    "        tt_start = np.where(t==mhw['time_start'][ev])[0][0]\n",
    "        tt_end = np.where(t==mhw['time_end'][ev])[0][0]\n",
    "        mhw['index_start'].append(tt_start)\n",
    "        mhw['index_end'].append(tt_end)\n",
    "        temp_mhw = temp[tt_start:tt_end+1]\n",
    "        thresh_mhw = clim['thresh'][tt_start:tt_end+1]\n",
    "        seas_mhw = clim['seas'][tt_start:tt_end+1]\n",
    "        mhw_relSeas = temp_mhw - seas_mhw\n",
    "        mhw_relThresh = temp_mhw - thresh_mhw\n",
    "        mhw_relThreshNorm = (temp_mhw - thresh_mhw) / (thresh_mhw - seas_mhw)\n",
    "        mhw_abs = temp_mhw\n",
    "        # Find peak\n",
    "        tt_peak = np.argmax(mhw_relSeas)\n",
    "        mhw['time_peak'].append(mhw['time_start'][ev] + tt_peak)\n",
    "        mhw['date_peak'].append(date.fromordinal(mhw['time_start'][ev] + tt_peak))\n",
    "        mhw['index_peak'].append(tt_start + tt_peak)\n",
    "        # MHW Duration\n",
    "        mhw['duration'].append(len(mhw_relSeas))\n",
    "        # MHW Intensity metrics\n",
    "        mhw['intensity_max'].append(mhw_relSeas[tt_peak])\n",
    "        mhw['intensity_mean'].append(mhw_relSeas.mean())\n",
    "        mhw['intensity_var'].append(np.sqrt(mhw_relSeas.var()))\n",
    "        mhw['intensity_cumulative'].append(mhw_relSeas.sum())\n",
    "        mhw['intensity_max_relThresh'].append(mhw_relThresh[tt_peak])\n",
    "        mhw['intensity_mean_relThresh'].append(mhw_relThresh.mean())\n",
    "        mhw['intensity_var_relThresh'].append(np.sqrt(mhw_relThresh.var()))\n",
    "        mhw['intensity_cumulative_relThresh'].append(mhw_relThresh.sum())\n",
    "        mhw['intensity_max_abs'].append(mhw_abs[tt_peak])\n",
    "        mhw['intensity_mean_abs'].append(mhw_abs.mean())\n",
    "        mhw['intensity_var_abs'].append(np.sqrt(mhw_abs.var()))\n",
    "        mhw['intensity_cumulative_abs'].append(mhw_abs.sum())\n",
    "        # Fix categories\n",
    "        tt_peakCat = np.argmax(mhw_relThreshNorm)\n",
    "        cats = np.floor(1. + mhw_relThreshNorm)\n",
    "        mhw['category'].append(categories[np.min([cats[tt_peakCat], 4]).astype(int) - 1])\n",
    "        mhw['duration_moderate'].append(np.sum(cats == 1.))\n",
    "        mhw['duration_strong'].append(np.sum(cats == 2.))\n",
    "        mhw['duration_severe'].append(np.sum(cats == 3.))\n",
    "        mhw['duration_extreme'].append(np.sum(cats >= 4.))\n",
    "        \n",
    "        # Rates of onset and decline\n",
    "        # Requires getting MHW strength at \"start\" and \"end\" of event (continuous: assume start/end half-day before/after first/last point)\n",
    "        if tt_start > 0:\n",
    "            mhw_relSeas_start = 0.5*(mhw_relSeas[0] + temp[tt_start-1] - clim['seas'][tt_start-1])\n",
    "            mhw['rate_onset'].append((mhw_relSeas[tt_peak] - mhw_relSeas_start) / (tt_peak+0.5))\n",
    "        else: # MHW starts at beginning of time series\n",
    "            if tt_peak == 0: # Peak is also at begining of time series, assume onset time = 1 day\n",
    "                mhw['rate_onset'].append((mhw_relSeas[tt_peak] - mhw_relSeas[0]) / 1.)\n",
    "            else:\n",
    "                mhw['rate_onset'].append((mhw_relSeas[tt_peak] - mhw_relSeas[0]) / tt_peak)\n",
    "        if tt_end < T-1:\n",
    "            mhw_relSeas_end = 0.5*(mhw_relSeas[-1] + temp[tt_end+1] - clim['seas'][tt_end+1])\n",
    "            mhw['rate_decline'].append((mhw_relSeas[tt_peak] - mhw_relSeas_end) / (tt_end-tt_start-tt_peak+0.5))\n",
    "        else: # MHW finishes at end of time series\n",
    "            if tt_peak == T-1: # Peak is also at end of time series, assume decline time = 1 day\n",
    "                mhw['rate_decline'].append((mhw_relSeas[tt_peak] - mhw_relSeas[-1]) / 1.)\n",
    "            else:\n",
    "                mhw['rate_decline'].append((mhw_relSeas[tt_peak] - mhw_relSeas[-1]) / (tt_end-tt_start-tt_peak))\n",
    "\n",
    "    # Flip climatology and intensties in case of cold spell detection\n",
    "    if coldSpells:\n",
    "        clim['seas'] = -1.*clim['seas']\n",
    "        clim['thresh'] = -1.*clim['thresh']\n",
    "        for ev in range(len(mhw['intensity_max'])):\n",
    "            mhw['intensity_max'][ev] = -1.*mhw['intensity_max'][ev]\n",
    "            mhw['intensity_mean'][ev] = -1.*mhw['intensity_mean'][ev]\n",
    "            mhw['intensity_cumulative'][ev] = -1.*mhw['intensity_cumulative'][ev]\n",
    "            mhw['intensity_max_relThresh'][ev] = -1.*mhw['intensity_max_relThresh'][ev]\n",
    "            mhw['intensity_mean_relThresh'][ev] = -1.*mhw['intensity_mean_relThresh'][ev]\n",
    "            mhw['intensity_cumulative_relThresh'][ev] = -1.*mhw['intensity_cumulative_relThresh'][ev]\n",
    "            mhw['intensity_max_abs'][ev] = -1.*mhw['intensity_max_abs'][ev]\n",
    "            mhw['intensity_mean_abs'][ev] = -1.*mhw['intensity_mean_abs'][ev]\n",
    "            mhw['intensity_cumulative_abs'][ev] = -1.*mhw['intensity_cumulative_abs'][ev]\n",
    "\n",
    "    return mhw, clim\n",
    "\n",
    "\n",
    "def blockAverage(t, mhw, clim=None, blockLength=1, removeMissing=False, temp=None):\n",
    "    '''\n",
    "    Calculate statistics of marine heatwave (MHW) properties averaged over blocks of\n",
    "    a specified length of time. Takes as input a collection of detected MHWs\n",
    "    (using the marineHeatWaves.detect function) and a time vector for the source\n",
    "    SST series.\n",
    "    Inputs:\n",
    "      t       Time vector, in datetime format (e.g., date(1982,1,1).toordinal())\n",
    "      mhw     Marine heat waves (MHWs) detected using marineHeatWaves.detect\n",
    "    Outputs:\n",
    "      mhwBlock   Time series of block-averaged MHW properties. Each key (following list)\n",
    "                 is a list of length N where N is the number of blocks:\n",
    " \n",
    "        'years_start'          Start year blocks (inclusive)\n",
    "        'years_end'            End year of blocks (inclusive)\n",
    "        'years_centre'         Decimal year at centre of blocks\n",
    "        'count'                Total MHW count in each block\n",
    "        'duration'             Average MHW duration in each block [days]\n",
    "        'intensity_max'        Average MHW \"maximum (peak) intensity\" in each block [deg. C]\n",
    "        'intensity_max_max'    Maximum MHW \"maximum (peak) intensity\" in each block [deg. C]\n",
    "        'intensity_mean'       Average MHW \"mean intensity\" in each block [deg. C]\n",
    "        'intensity_var'        Average MHW \"intensity variability\" in each block [deg. C]\n",
    "        'intensity_cumulative' Average MHW \"cumulative intensity\" in each block [deg. C x days]\n",
    "        'rate_onset'           Average MHW onset rate in each block [deg. C / days]\n",
    "        'rate_decline'         Average MHW decline rate in each block [deg. C / days]\n",
    "        'total_days'           Total number of MHW days in each block [days]\n",
    "        'total_icum'           Total cumulative intensity over all MHWs in each block [deg. C x days]\n",
    "        'intensity_max_relThresh', 'intensity_mean_relThresh', 'intensity_var_relThresh', \n",
    "        and 'intensity_cumulative_relThresh' are as above except relative to the\n",
    "        threshold (e.g., 90th percentile) rather than the seasonal climatology\n",
    "        'intensity_max_abs', 'intensity_mean_abs', 'intensity_var_abs', and\n",
    "        'intensity_cumulative_abs' are as above except as absolute magnitudes\n",
    "        rather than relative to the seasonal climatology or threshold\n",
    "    Options:\n",
    "      blockLength            Size of block (in years) over which to calculate the\n",
    "                             averaged MHW properties. Must be an integer greater than\n",
    "                             or equal to 1 (DEFAULT = 1 [year])\n",
    "      removeMissing          Boolean switch indicating whether to remove (set = NaN)\n",
    "                             statistics for any blocks in which there were missing \n",
    "                             temperature values (DEFAULT = FALSE)\n",
    "      clim                   The temperature climatology (including missing value information)\n",
    "                             as output by marineHeatWaves.detect (required if removeMissing = TRUE)\n",
    "      temp                   Temperature time series. If included mhwBlock will output block\n",
    "                             averages of mean, max, and min temperature (DEFAULT = NONE)\n",
    "                             If both clim and temp are provided, this will output annual counts\n",
    "                             of moderate, strong, severe, and extreme days.\n",
    "    Notes:\n",
    "      This function assumes that the input time vector consists of continuous daily values. Note that\n",
    "      in the case of time ranges which start and end part-way through the calendar year, the block\n",
    "      averages at the endpoints, for which there is less than a block length of data, will need to be\n",
    "      interpreted with care.\n",
    "    Written by Eric Oliver, Institue for Marine and Antarctic Studies, University of Tasmania, Feb-Mar 2015\n",
    "    '''\n",
    "\n",
    "    #\n",
    "    # Time and dates vectors, and calculate block timing\n",
    "    #\n",
    "\n",
    "    # Generate vectors for year, month, day-of-month, and day-of-year\n",
    "    T = len(t)\n",
    "    year = np.zeros((T))\n",
    "    month = np.zeros((T))\n",
    "    day = np.zeros((T))\n",
    "    for i in range(T):\n",
    "        year[i] = date.fromordinal(t[i]).year\n",
    "        month[i] = date.fromordinal(t[i]).month\n",
    "        day[i] = date.fromordinal(t[i]).day\n",
    "\n",
    "    # Number of blocks, round up to include partial blocks at end\n",
    "    years = np.unique(year)\n",
    "    nBlocks = np.ceil((years.max() - years.min() + 1) / blockLength).astype(int)\n",
    "\n",
    "    #\n",
    "    # Temperature time series included?\n",
    "    #\n",
    "\n",
    "    sw_temp = None\n",
    "    sw_cats = None\n",
    "    if temp is not None:\n",
    "        sw_temp = True\n",
    "        if clim is not None:\n",
    "            sw_cats = True\n",
    "        else:\n",
    "            sw_cats = False\n",
    "    else:\n",
    "        sw_temp = False\n",
    "\n",
    "    #\n",
    "    # Initialize MHW output variable\n",
    "    #\n",
    "\n",
    "    mhwBlock = {}\n",
    "    mhwBlock['count'] = np.zeros(nBlocks)\n",
    "    mhwBlock['count'] = np.zeros(nBlocks)\n",
    "    mhwBlock['duration'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_max'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_max_max'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_mean'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_cumulative'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_var'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_max_relThresh'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_mean_relThresh'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_cumulative_relThresh'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_var_relThresh'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_max_abs'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_mean_abs'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_cumulative_abs'] = np.zeros(nBlocks)\n",
    "    mhwBlock['intensity_var_abs'] = np.zeros(nBlocks)\n",
    "    mhwBlock['rate_onset'] = np.zeros(nBlocks)\n",
    "    mhwBlock['rate_decline'] = np.zeros(nBlocks)\n",
    "    mhwBlock['total_days'] = np.zeros(nBlocks)\n",
    "    mhwBlock['total_icum'] = np.zeros(nBlocks)\n",
    "    if sw_temp:\n",
    "        mhwBlock['temp_mean'] = np.zeros(nBlocks)\n",
    "        mhwBlock['temp_max'] = np.zeros(nBlocks)\n",
    "        mhwBlock['temp_min'] = np.zeros(nBlocks)\n",
    "\n",
    "    # Calculate category days\n",
    "    if sw_cats:\n",
    "        mhwBlock['moderate_days'] = np.zeros(nBlocks)\n",
    "        mhwBlock['strong_days'] = np.zeros(nBlocks)\n",
    "        mhwBlock['severe_days'] = np.zeros(nBlocks)\n",
    "        mhwBlock['extreme_days'] = np.zeros(nBlocks)\n",
    "        cats = np.floor(1 + (temp - clim['thresh']) / (clim['thresh'] - clim['seas']))\n",
    "        mhwIndex = np.zeros(t.shape)\n",
    "        for ev in range(mhw['n_events']):\n",
    "            mhwIndex[mhw['index_start'][ev]:mhw['index_end'][ev]+1] = 1.\n",
    "\n",
    "\n",
    "    # Start, end, and centre years for all blocks\n",
    "    mhwBlock['years_start'] = years[range(0, len(years), blockLength)]\n",
    "    mhwBlock['years_end'] = mhwBlock['years_start'] + blockLength - 1\n",
    "    mhwBlock['years_centre'] = 0.5*(mhwBlock['years_start'] + mhwBlock['years_end'])\n",
    "\n",
    "    #\n",
    "    # Calculate block averages\n",
    "    #\n",
    "\n",
    "    for i in range(mhw['n_events']):\n",
    "        # Block index for year of each MHW (MHW year defined by start year)\n",
    "        iBlock = np.where((mhwBlock['years_start'] <= mhw['date_start'][i].year) * (mhwBlock['years_end'] >= mhw['date_start'][i].year))[0][0]\n",
    "        # Add MHW properties to block count\n",
    "        mhwBlock['count'][iBlock] += 1\n",
    "        mhwBlock['duration'][iBlock] += mhw['duration'][i]\n",
    "        mhwBlock['intensity_max'][iBlock] += mhw['intensity_max'][i]\n",
    "        mhwBlock['intensity_max_max'][iBlock] = np.max([mhwBlock['intensity_max_max'][iBlock], mhw['intensity_max'][i]])\n",
    "        mhwBlock['intensity_mean'][iBlock] += mhw['intensity_mean'][i]\n",
    "        mhwBlock['intensity_cumulative'][iBlock] += mhw['intensity_cumulative'][i]\n",
    "        mhwBlock['intensity_var'][iBlock] += mhw['intensity_var'][i]\n",
    "        mhwBlock['intensity_max_relThresh'][iBlock] += mhw['intensity_max_relThresh'][i]\n",
    "        mhwBlock['intensity_mean_relThresh'][iBlock] += mhw['intensity_mean_relThresh'][i]\n",
    "        mhwBlock['intensity_cumulative_relThresh'][iBlock] += mhw['intensity_cumulative_relThresh'][i]\n",
    "        mhwBlock['intensity_var_relThresh'][iBlock] += mhw['intensity_var_relThresh'][i]\n",
    "        mhwBlock['intensity_max_abs'][iBlock] += mhw['intensity_max_abs'][i]\n",
    "        mhwBlock['intensity_mean_abs'][iBlock] += mhw['intensity_mean_abs'][i]\n",
    "        mhwBlock['intensity_cumulative_abs'][iBlock] += mhw['intensity_cumulative_abs'][i]\n",
    "        mhwBlock['intensity_var_abs'][iBlock] += mhw['intensity_var_abs'][i]\n",
    "        mhwBlock['rate_onset'][iBlock] += mhw['rate_onset'][i]\n",
    "        mhwBlock['rate_decline'][iBlock] += mhw['rate_decline'][i]\n",
    "        if mhw['date_start'][i].year == mhw['date_end'][i].year: # MHW in single year\n",
    "            mhwBlock['total_days'][iBlock] += mhw['duration'][i]\n",
    "        else: # MHW spans multiple years\n",
    "            year_mhw = year[mhw['index_start'][i]:mhw['index_end'][i]+1]\n",
    "            for yr_mhw in np.unique(year_mhw):\n",
    "                iBlock = np.where((mhwBlock['years_start'] <= yr_mhw) * (mhwBlock['years_end'] >= yr_mhw))[0][0]\n",
    "                mhwBlock['total_days'][iBlock] += np.sum(year_mhw == yr_mhw)\n",
    "        # NOTE: icum for a MHW is assigned to its start year, even if it spans mult. years\n",
    "        mhwBlock['total_icum'][iBlock] += mhw['intensity_cumulative'][i]\n",
    "\n",
    "    # Calculation of category days\n",
    "    if sw_cats:\n",
    "        for i in range(int(nBlocks)):\n",
    "            mhwBlock['moderate_days'][i] = ((year >= mhwBlock['years_start'][i]) * (year <= mhwBlock['years_end'][i]) * mhwIndex * (cats == 1)).astype(int).sum()\n",
    "            mhwBlock['strong_days'][i] = ((year >= mhwBlock['years_start'][i]) * (year <= mhwBlock['years_end'][i]) * mhwIndex * (cats == 2)).astype(int).sum()\n",
    "            mhwBlock['severe_days'][i] = ((year >= mhwBlock['years_start'][i]) * (year <= mhwBlock['years_end'][i]) * mhwIndex * (cats == 3)).astype(int).sum()\n",
    "            mhwBlock['extreme_days'][i] = ((year >= mhwBlock['years_start'][i]) * (year <= mhwBlock['years_end'][i]) * mhwIndex * (cats >= 4)).astype(int).sum()\n",
    "\n",
    "    # Calculate averages\n",
    "    count = 1.*mhwBlock['count']\n",
    "    count[count==0] = np.nan\n",
    "    mhwBlock['duration'] = mhwBlock['duration'] / count\n",
    "    mhwBlock['intensity_max'] = mhwBlock['intensity_max'] / count\n",
    "    mhwBlock['intensity_mean'] = mhwBlock['intensity_mean'] / count\n",
    "    mhwBlock['intensity_cumulative'] = mhwBlock['intensity_cumulative'] / count\n",
    "    mhwBlock['intensity_var'] = mhwBlock['intensity_var'] / count\n",
    "    mhwBlock['intensity_max_relThresh'] = mhwBlock['intensity_max_relThresh'] / count\n",
    "    mhwBlock['intensity_mean_relThresh'] = mhwBlock['intensity_mean_relThresh'] / count\n",
    "    mhwBlock['intensity_cumulative_relThresh'] = mhwBlock['intensity_cumulative_relThresh'] / count\n",
    "    mhwBlock['intensity_var_relThresh'] = mhwBlock['intensity_var_relThresh'] / count\n",
    "    mhwBlock['intensity_max_abs'] = mhwBlock['intensity_max_abs'] / count\n",
    "    mhwBlock['intensity_mean_abs'] = mhwBlock['intensity_mean_abs'] / count\n",
    "    mhwBlock['intensity_cumulative_abs'] = mhwBlock['intensity_cumulative_abs'] / count\n",
    "    mhwBlock['intensity_var_abs'] = mhwBlock['intensity_var_abs'] / count\n",
    "    mhwBlock['rate_onset'] = mhwBlock['rate_onset'] / count\n",
    "    mhwBlock['rate_decline'] = mhwBlock['rate_decline'] / count\n",
    "    # Replace empty years in intensity_max_max\n",
    "    mhwBlock['intensity_max_max'][np.isnan(mhwBlock['intensity_max'])] = np.nan\n",
    "\n",
    "    # Temperature series\n",
    "    if sw_temp:\n",
    "        for i in range(int(nBlocks)):\n",
    "            tt = (year >= mhwBlock['years_start'][i]) * (year <= mhwBlock['years_end'][i])\n",
    "            mhwBlock['temp_mean'][i] = np.nanmean(temp[tt])\n",
    "            mhwBlock['temp_max'][i] = np.nanmax(temp[tt])\n",
    "            mhwBlock['temp_min'][i] = np.nanmin(temp[tt])\n",
    "\n",
    "    #\n",
    "    # Remove years with missing values\n",
    "    #\n",
    "\n",
    "    if removeMissing:\n",
    "        missingYears = np.unique(year[np.where(clim['missing'])[0]])\n",
    "        for y in range(len(missingYears)):\n",
    "            iMissing = np.where((mhwBlock['years_start'] <= missingYears[y]) * (mhwBlock['years_end'] >= missingYears[y]))[0][0]\n",
    "            mhwBlock['count'][iMissing] = np.nan\n",
    "            mhwBlock['duration'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_max'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_max_max'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_mean'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_cumulative'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_var'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_max_relThresh'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_mean_relThresh'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_cumulative_relThresh'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_var_relThresh'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_max_abs'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_mean_abs'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_cumulative_abs'][iMissing] = np.nan\n",
    "            mhwBlock['intensity_var_abs'][iMissing] = np.nan\n",
    "            mhwBlock['rate_onset'][iMissing] = np.nan\n",
    "            mhwBlock['rate_decline'][iMissing] = np.nan\n",
    "            mhwBlock['total_days'][iMissing] = np.nan\n",
    "            if sw_cats:\n",
    "                mhwBlock['moderate_days'][iMissing] = np.nan\n",
    "                mhwBlock['strong_days'][iMissing] = np.nan\n",
    "                mhwBlock['severe_days'][iMissing] = np.nan\n",
    "                mhwBlock['extreme_days'][iMissing] = np.nan\n",
    "            mhwBlock['total_icum'][iMissing] = np.nan\n",
    "\n",
    "    return mhwBlock\n",
    "\n",
    "\n",
    "def meanTrend(mhwBlock, alpha=0.05):\n",
    "    '''\n",
    "    Calculates the mean and trend of marine heatwave (MHW) properties. Takes as input a\n",
    "    collection of block-averaged MHW properties (using the marineHeatWaves.blockAverage\n",
    "    function). Handles missing values (which should be specified by NaNs).\n",
    "    Inputs:\n",
    "      mhwBlock      Time series of block-averaged MHW statistics calculated using the\n",
    "                    marineHeatWaves.blockAverage function\n",
    "      alpha         Significance level for estimate of confidence limits on trend, e.g.,\n",
    "                    alpha = 0.05 for 5% significance (or 95% confidence) (DEFAULT = 0.05)\n",
    "    Outputs:\n",
    "      mean          Mean of all MHW properties over all block-averaged values\n",
    "      trend         Linear trend of all MHW properties over all block-averaged values\n",
    "      dtrend        One-sided width of (1-alpha)% confidence intevfal on linear trend,\n",
    "                    i.e., trend lies within (trend-dtrend, trend+dtrend) with specified\n",
    "                    level  of confidence.\n",
    "                    Both mean and trend have the following keys, the units the trend\n",
    "                    are the units of the property of interest per year:\n",
    "        'duration'             Duration of MHW [days]\n",
    "        'intensity_max'        Maximum (peak) intensity [deg. C]\n",
    "        'intensity_mean'       Mean intensity [deg. C]\n",
    "        'intensity_var'        Intensity variability [deg. C]\n",
    "        'intensity_cumulative' Cumulative intensity [deg. C x days]\n",
    "        'rate_onset'           Onset rate of MHW [deg. C / days]\n",
    "        'rate_decline'         Decline rate of MHW [deg. C / days]\n",
    "        'intensity_max_relThresh', 'intensity_mean_relThresh', 'intensity_var_relThresh', \n",
    "        and 'intensity_cumulative_relThresh' are as above except relative to the\n",
    "        threshold (e.g., 90th percentile) rather than the seasonal climatology\n",
    "        'intensity_max_abs', 'intensity_mean_abs', 'intensity_var_abs', and\n",
    "        'intensity_cumulative_abs' are as above except as absolute magnitudes\n",
    "        rather than relative to the seasonal climatology or threshold\n",
    "    Notes:\n",
    "      This calculation performs a multiple linear regression of the form\n",
    "        y ~ beta * X + eps\n",
    "      where y is the MHW property of interest and X is a matrix of predictors. The first\n",
    "      column of X is all ones to estimate the mean, the second column is the time vector\n",
    "      which is taken as mhwBlock['years_centre'] and offset to be equal to zero at its\n",
    "      mid-point.\n",
    "    Written by Eric Oliver, Institue for Marine and Antarctic Studies, University of Tasmania, Feb-Mar 2015\n",
    "    '''\n",
    "\n",
    "    # Initialize mean and trend dictionaries\n",
    "    mean = {}\n",
    "    trend = {}\n",
    "    dtrend = {}\n",
    "\n",
    "    # Construct matrix of predictors, first column is all ones to estimate the mean,\n",
    "    # second column is the time vector, equal to zero at mid-point.\n",
    "    t = mhwBlock['years_centre']\n",
    "    X = np.array([np.ones(t.shape), t-t.mean()]).T\n",
    "\n",
    "    # Loop over all keys in mhwBlock\n",
    "    for key in mhwBlock.keys():\n",
    "        # Skip time-vector keys of mhwBlock\n",
    "        if (key == 'years_centre') + (key == 'years_end') + (key == 'years_start'):\n",
    "            continue\n",
    "\n",
    "        # Predictand (MHW property of interest)\n",
    "        y = mhwBlock[key]\n",
    "        valid = ~np.isnan(y) # non-NaN indices\n",
    "\n",
    "        # Perform linear regression over valid indices\n",
    "        if np.isinf(nonans(y).sum()): # If contains Inf values\n",
    "            beta = [np.nan, np.nan]\n",
    "        elif np.sum(~np.isnan(y)) > 0: # If at least one non-NaN value\n",
    "            beta = linalg.lstsq(X[valid,:], y[valid])[0]\n",
    "        else:\n",
    "            beta = [np.nan, np.nan]\n",
    "\n",
    "        # Insert regression coefficients into mean and trend dictionaries\n",
    "        mean[key] = beta[0]\n",
    "        trend[key] = beta[1]\n",
    "\n",
    "        # Confidence limits on trend\n",
    "        yhat = np.sum(beta*X, axis=1)\n",
    "        t_stat = stats.t.isf(alpha/2, len(t[valid])-2)\n",
    "        s = np.sqrt(np.sum((y[valid] - yhat[valid])**2) / (len(t[valid])-2))\n",
    "        Sxx = np.sum(X[valid,1]**2) - (np.sum(X[valid,1])**2)/len(t[valid]) # np.var(X, axis=1)[1]\n",
    "        dbeta1 = t_stat * s / np.sqrt(Sxx)\n",
    "        dtrend[key] = dbeta1\n",
    "\n",
    "    # Return mean, trend\n",
    "    return mean, trend, dtrend\n",
    "\n",
    "\n",
    "def rank(t, mhw):\n",
    "    '''\n",
    "    Calculate the rank and return periods of marine heatwaves (MHWs) according to\n",
    "    each metric. Takes as input a collection of detected MHWs (using the\n",
    "    marineHeatWaves.detect function) and a time vector for the source SST series.\n",
    "    Inputs:\n",
    "      t       Time vector, in datetime format (e.g., date(1982,1,1).toordinal())\n",
    "      mhw     Marine heat waves (MHWs) detected using marineHeatWaves.detect\n",
    "    Outputs:\n",
    "      rank          The rank of each MHW according to each MHW property. A rank of 1 is the\n",
    "                    largest, 2 is the 2nd largest, etc. Each key (listed below) is a list\n",
    "                    of length N where N is the number of MHWs.\n",
    "      returnPeriod  The return period (in years) of each MHW according to each MHW property.\n",
    "                    The return period signifies, statistically, the recurrence interval for\n",
    "                    an event at least as large/long as the event in quetion. Each key (listed\n",
    "                    below) is a list of length N where N is the number of MHWs.\n",
    " \n",
    "        'duration'             Average MHW duration in each block [days]\n",
    "        'intensity_max'        Average MHW \"maximum (peak) intensity\" in each block [deg. C]\n",
    "        'intensity_mean'       Average MHW \"mean intensity\" in each block [deg. C]\n",
    "        'intensity_var'        Average MHW \"intensity variability\" in each block [deg. C]\n",
    "        'intensity_cumulative' Average MHW \"cumulative intensity\" in each block [deg. C x days]\n",
    "        'rate_onset'           Average MHW onset rate in each block [deg. C / days]\n",
    "        'rate_decline'         Average MHW decline rate in each block [deg. C / days]\n",
    "        'total_days'           Total number of MHW days in each block [days]\n",
    "        'total_icum'           Total cumulative intensity over all MHWs in each block [deg. C x days]\n",
    "        'intensity_max_relThresh', 'intensity_mean_relThresh', 'intensity_var_relThresh', \n",
    "        and 'intensity_cumulative_relThresh' are as above except relative to the\n",
    "        threshold (e.g., 90th percentile) rather than the seasonal climatology\n",
    "        'intensity_max_abs', 'intensity_mean_abs', 'intensity_var_abs', and\n",
    "        'intensity_cumulative_abs' are as above except as absolute magnitudes\n",
    "        rather than relative to the seasonal climatology or threshold\n",
    "    Notes:\n",
    "      This function assumes that the MHWs were calculated over a suitably long record that return\n",
    "      periods make sense. If the record length is a few years or less than this becomes meaningless.\n",
    "    Written by Eric Oliver, Institue for Marine and Antarctic Studies, University of Tasmania, Sep 2015\n",
    "    '''\n",
    "\n",
    "    # Initialize rank and return period dictionaries\n",
    "    rank = {}\n",
    "    returnPeriod = {}\n",
    "\n",
    "    # Number of years on record\n",
    "    nYears = len(t)/365.25\n",
    "\n",
    "    # Loop over all keys in mhw\n",
    "    for key in mhw.keys():\n",
    "        # Skip irrelevant keys of mhw, only calculate rank/returns for MHW properties\n",
    "        if (key == 'date_end') + (key == 'date_peak') + (key == 'date_start') + (key == 'date_end') + (key == 'date_peak') + (key == 'date_start') + (key == 'index_end') + (key == 'index_peak') + (key == 'index_start') + (key == 'n_events'):\n",
    "            continue\n",
    "\n",
    "        # Calculate ranks\n",
    "        rank[key] = mhw['n_events'] - np.array(mhw[key]).argsort().argsort()  \n",
    "        # Calculate return period as (# years on record + 1) / (# of occurrences of event)\n",
    "        # Return period is for events of at least the event magnitude/duration\n",
    "        returnPeriod[key] = (nYears + 1) / rank[key]\n",
    "\n",
    "    # Return rank, return\n",
    "    return rank, returnPeriod\n",
    "\n",
    "\n",
    "def runavg(ts, w):\n",
    "    '''\n",
    "    Performs a running average of an input time series using uniform window\n",
    "    of width w. This function assumes that the input time series is periodic.\n",
    "    Inputs:\n",
    "      ts            Time series [1D numpy array]\n",
    "      w             Integer length (must be odd) of running average window\n",
    "    Outputs:\n",
    "      ts_smooth     Smoothed time series\n",
    "    Written by Eric Oliver, Institue for Marine and Antarctic Studies, University of Tasmania, Feb-Mar 2015\n",
    "    '''\n",
    "    # Original length of ts\n",
    "    N = len(ts)\n",
    "    # make ts three-fold periodic\n",
    "    ts = np.append(ts, np.append(ts, ts))\n",
    "    # smooth by convolution with a window of equal weights\n",
    "    ts_smooth = np.convolve(ts, np.ones(w)/w, mode='same')\n",
    "    # Only output central section, of length equal to the original length of ts\n",
    "    ts = ts_smooth[N:2*N]\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "def pad(data, maxPadLength=False):\n",
    "    '''\n",
    "    Linearly interpolate over missing data (NaNs) in a time series.\n",
    "    Inputs:\n",
    "      data\t     Time series [1D numpy array]\n",
    "      maxPadLength   Specifies the maximum length over which to interpolate,\n",
    "                     i.e., any consecutive blocks of NaNs with length greater\n",
    "                     than maxPadLength will be left as NaN. Set as an integer.\n",
    "                     maxPadLength=False (default) interpolates over all NaNs.\n",
    "    Written by Eric Oliver, Institue for Marine and Antarctic Studies, University of Tasmania, Jun 2015\n",
    "    '''\n",
    "    data_padded = data.copy()\n",
    "    bad_indexes = np.isnan(data)\n",
    "    good_indexes = np.logical_not(bad_indexes)\n",
    "    good_data = data[good_indexes]\n",
    "    interpolated = np.interp(bad_indexes.nonzero()[0], good_indexes.nonzero()[0], good_data)\n",
    "    data_padded[bad_indexes] = interpolated\n",
    "    if maxPadLength:\n",
    "        blocks, n_blocks = ndimage.label(np.isnan(data))\n",
    "        for bl in range(1, n_blocks+1):\n",
    "            if (blocks==bl).sum() > maxPadLength:\n",
    "                data_padded[blocks==bl] = np.nan\n",
    "\n",
    "    return data_padded\n",
    "\n",
    "\n",
    "def nonans(array):\n",
    "    '''\n",
    "    Return input array [1D numpy array] with\n",
    "    all nan values removed\n",
    "    '''\n",
    "    return array[~np.isnan(array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b59e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.509999\n"
     ]
    }
   ],
   "source": [
    "t = np.arange(date(1982,1,1).toordinal(),date(2021,12,31).toordinal()+1)\n",
    "dates = [date.fromordinal(tt.astype(int)) for tt in t]\n",
    "# Generate synthetic temperature time series\n",
    "\n",
    "data = np.load(r'D:\\heat_wave\\atlantic\\SST_82_21_expand_atlantic_area.npz')['sst']\n",
    "# print(data.shape) #(9861, 30, 41, 201)\n",
    "# data = data[:,0,:,:]\n",
    "print(np.min(data))\n",
    "\n",
    "sst = np.mean(data, axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f63416c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.67506796, 24.71304955, 24.75075857, ..., 24.55987124,\n",
       "       24.59842202, 24.63684353])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhws, clim = detect(t, sst)\n",
    "clim['seas'].shape\n",
    "data1 = clim['seas']\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da4a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#假设clim['seas']为气候态的平均值\n",
    "#需要1982/83   1987/88  1997/98  2002/03  2004/05  2009/10  2015/16  2018/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c101cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim1 = np.datetime64('1982-01-01')\n",
    "end_date_clim1 = np.datetime64('1983-12-31')\n",
    "start_index_clim1 = np.where(dates == start_date_clim1)[0][0]\n",
    "end_index_clim1 = np.where(dates == end_date_clim1)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim1 = data1[start_index_clim1:end_index_clim1]\n",
    "target_data_clim_82_83 = target_data_clim1\n",
    "print(np.mean(target_data_clim_82_83))  # (730, 82, 82)\n",
    "target_Nino_data_clim_82_83 = target_data_clim_82_83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e54e888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306,)\n",
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的1987/88的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim2 = np.datetime64('1987-01-01')\n",
    "end_date_clim2 = np.datetime64('1988-02-28')\n",
    "start_index_clim2 = np.where(dates == start_date_clim2)[0][0]\n",
    "end_index_clim2 = np.where(dates == end_date_clim2)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim2 = data1[start_index_clim2:end_index_clim2]\n",
    "\n",
    "\n",
    "start_date_clim22 = np.datetime64('1988-03-01')\n",
    "end_date_clim22 = np.datetime64('1988-12-31')\n",
    "start_index_clim22 = np.where(dates == start_date_clim22)[0][0]\n",
    "end_index_clim22 = np.where(dates == end_date_clim22)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim22 = data1[start_index_clim22:end_index_clim22]\n",
    "\n",
    "print(target_data_clim22.shape)  # (306, 82, 82)\n",
    "\n",
    "\n",
    "target_data_clim2 = np.concatenate((target_data_clim2, target_data_clim22), axis = 0)\n",
    "target_data_clim_87_88 = target_data_clim2\n",
    "print(np.mean(target_data_clim_87_88)) \n",
    "target_data_clim_87_88 = target_data_clim_87_88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55a0c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的1997/98的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim3 = np.datetime64('1997-01-01')\n",
    "end_date_clim3 = np.datetime64('1998-12-31')\n",
    "start_index_clim3 = np.where(dates == start_date_clim3)[0][0]\n",
    "end_index_clim3 = np.where(dates == end_date_clim3)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim3 = data1[start_index_clim3:end_index_clim3]\n",
    "target_data_clim_97_98 = target_data_clim3\n",
    "print(np.mean(target_data_clim_97_98)) \n",
    "target_data_clim_97_98 = target_data_clim_97_98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb40d9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的2002/03的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim4 = np.datetime64('2002-01-01')\n",
    "end_date_clim4 = np.datetime64('2003-12-31')\n",
    "start_index_clim4 = np.where(dates == start_date_clim4)[0][0]\n",
    "end_index_clim4 = np.where(dates == end_date_clim4)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim4 = data1[start_index_clim4:end_index_clim4]\n",
    "target_data_clim_02_03 = target_data_clim4\n",
    "print(np.mean(target_data_clim_02_03)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d061ee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671,)\n",
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的2004/05的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim5 = np.datetime64('2004-01-01')\n",
    "end_date_clim5 = np.datetime64('2004-02-28')\n",
    "start_index_clim5 = np.where(dates == start_date_clim5)[0][0]\n",
    "end_index_clim5 = np.where(dates == end_date_clim5)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim5 = data1[start_index_clim5:end_index_clim5]\n",
    "\n",
    "# print(target_data5.shape)  # (730, 82, 82)\n",
    "\n",
    "start_date_clim55 = np.datetime64('2004-03-01')\n",
    "end_date_clim55 = np.datetime64('2005-12-31')\n",
    "start_index_clim55 = np.where(dates == start_date_clim55)[0][0]\n",
    "end_index_clim55 = np.where(dates == end_date_clim55)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim55 = data1[start_index_clim55:end_index_clim55]\n",
    "\n",
    "print(target_data_clim55.shape)  # (730, 82, 82)\n",
    "\n",
    "target_data_clim5 = np.concatenate((target_data_clim5, target_data_clim55), axis = 0)\n",
    "target_data_clim_04_05 = target_data_clim5\n",
    "print(np.mean(target_data_clim_04_05)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0305f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的2009/10的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim6 = np.datetime64('2009-01-01')\n",
    "end_date_clim6 = np.datetime64('2010-12-31')\n",
    "start_index_clim6 = np.where(dates == start_date_clim6)[0][0]\n",
    "end_index_clim6 = np.where(dates == end_date_clim6)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim6 = data1[start_index_clim6:end_index_clim6]\n",
    "target_data_clim_09_10 = target_data_clim6\n",
    "# print(target_data6.shape)  # (730, 82, 82)\n",
    "print(np.mean(target_data_clim_09_10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e76a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424,)\n",
      "(306,)\n",
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的2015/16的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim7 = np.datetime64('2015-01-01')\n",
    "end_date_clim7 = np.datetime64('2016-02-28')\n",
    "start_index_clim7 = np.where(dates == start_date_clim7)[0][0]\n",
    "end_index_clim7 = np.where(dates == end_date_clim7)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim7 = data1[start_index_clim7:end_index_clim7]\n",
    "\n",
    "print(target_data_clim7.shape)  # (730, 82, 82)\n",
    "\n",
    "\n",
    "start_date_clim77 = np.datetime64('2016-03-01')\n",
    "end_date_clim77 = np.datetime64('2016-12-31')\n",
    "start_index_clim77 = np.where(dates == start_date_clim77)[0][0]\n",
    "end_index_clim77 = np.where(dates == end_date_clim77)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim77 = data1[start_index_clim77:end_index_clim77]\n",
    "\n",
    "print(target_data_clim77.shape)  # (730, 82, 82)\n",
    "\n",
    "target_data_clim7 = np.concatenate((target_data_clim7, target_data_clim77), axis = 0)\n",
    "target_data_clim_15_16 = target_data_clim7\n",
    "# print(target_data7.shape)\n",
    "print(np.mean(target_data_clim_15_16)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f225e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.98290794396158\n"
     ]
    }
   ],
   "source": [
    "# 完整的2018/19的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_clim8 = np.datetime64('2018-01-01')\n",
    "end_date_clim8 = np.datetime64('2019-12-31')\n",
    "start_index_clim8 = np.where(dates == start_date_clim8)[0][0]\n",
    "end_index_clim8 = np.where(dates == end_date_clim8)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_clim8 = data1[start_index_clim8:end_index_clim8]\n",
    "target_data_clim_18_19 = target_data_clim8\n",
    "# print(target_data8.shape)  # (730, 82, 82)\n",
    "print(np.mean(target_data_clim_18_19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdfcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(r'D:\\heat_wave\\atlantic\\EINO_NINA_plot\\Nino_data_clim.npz', target_Nino_data_clim_82_83 = target_data_clim_82_83, target_Nino_data_clim_87_88 = target_data_clim_87_88,\n",
    "        target_Nino_data_clim_97_98 = target_data_clim_97_98, target_Nino_data_clim_02_03 = target_data_clim_02_03, target_Nino_data_clim_04_05 = target_data_clim_04_05,\n",
    "        target_Nino_data_clim_09_10 = target_data_clim_09_10, target_Nino_data_clim_15_16 = target_data_clim_15_16,\n",
    "        target_Nino_data_clim_18_19 = target_data_clim_18_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f7ac3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.98290794396158"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clim= np.mean([target_data_clim_82_83, target_data_clim_87_88, target_data_clim_97_98, target_data_clim_02_03,\n",
    "                        target_data_clim_02_03, target_data_clim_09_10, target_data_clim_15_16, target_data_clim_18_19], axis = 0)\n",
    "\n",
    "data_clim.shape\n",
    "np.mean(data_clim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6386610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14610, 82, 82)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(r'D:\\heat_wave\\atlantic\\SST_82_21_expand_atlantic_area.npz')['sst']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b215ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nino_year: 1982/83  1987/88  1997/98  2002/03   2004/05   2009/10   2015/16   2018/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac11b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.764635\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 完整的1982/83的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date1 = np.datetime64('1982-01-01')\n",
    "end_date1 = np.datetime64('1983-12-31')\n",
    "start_index1 = np.where(dates == start_date1)[0][0]\n",
    "end_index1 = np.where(dates == end_date1)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data1 = data[start_index1:end_index1]\n",
    "target_data_82_83 = target_data1\n",
    "print(np.mean(target_data_82_83))  # (730, 82, 82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37da2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 82, 82)\n",
      "(306, 82, 82)\n",
      "24.187937\n"
     ]
    }
   ],
   "source": [
    "# 完整的1987/88的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date2 = np.datetime64('1987-01-01')\n",
    "end_date2 = np.datetime64('1988-02-28')\n",
    "start_index2 = np.where(dates == start_date2)[0][0]\n",
    "end_index2 = np.where(dates == end_date2)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data2 = data[start_index2:end_index2]\n",
    "\n",
    "print(target_data2.shape)  # (424, 82, 82)\n",
    "\n",
    "start_date22 = np.datetime64('1988-03-01')\n",
    "end_date22 = np.datetime64('1988-12-31')\n",
    "start_index22 = np.where(dates == start_date22)[0][0]\n",
    "end_index22 = np.where(dates == end_date22)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data22 = data[start_index22:end_index22]\n",
    "\n",
    "print(target_data22.shape)  # (306, 82, 82)\n",
    "\n",
    "\n",
    "target_data2 = np.concatenate((target_data2, target_data22), axis = 0)\n",
    "target_data_87_88 = target_data2\n",
    "print(np.mean(target_data_87_88)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90fba1ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.805622\n"
     ]
    }
   ],
   "source": [
    "# 完整的1997/98的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date3 = np.datetime64('1997-01-01')\n",
    "end_date3 = np.datetime64('1998-12-31')\n",
    "start_index3 = np.where(dates == start_date3)[0][0]\n",
    "end_index3 = np.where(dates == end_date3)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data3 = data[start_index3:end_index3]\n",
    "target_data_97_98 = target_data3\n",
    "print(np.mean(target_data_97_98)) \n",
    "# target_data_97_98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49c7b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.258202\n"
     ]
    }
   ],
   "source": [
    "# 完整的2002/03的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date4 = np.datetime64('2002-01-01')\n",
    "end_date4 = np.datetime64('2003-12-31')\n",
    "start_index4 = np.where(dates == start_date4)[0][0]\n",
    "end_index4 = np.where(dates == end_date4)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data4 = data[start_index4:end_index4]\n",
    "target_data_02_03 = target_data4\n",
    "print(np.mean(target_data_02_03)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f03d0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 82, 82)\n",
      "(671, 82, 82)\n",
      "23.943996\n"
     ]
    }
   ],
   "source": [
    "# 完整的2004/05的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date5 = np.datetime64('2004-01-01')\n",
    "end_date5 = np.datetime64('2004-02-28')\n",
    "start_index5 = np.where(dates == start_date5)[0][0]\n",
    "end_index5 = np.where(dates == end_date5)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data5 = data[start_index5:end_index5]\n",
    "\n",
    "print(target_data5.shape)  # (730, 82, 82)\n",
    "\n",
    "start_date55 = np.datetime64('2004-03-01')\n",
    "end_date55 = np.datetime64('2005-12-31')\n",
    "start_index55 = np.where(dates == start_date55)[0][0]\n",
    "end_index55 = np.where(dates == end_date55)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data55 = data[start_index55:end_index55]\n",
    "\n",
    "print(target_data55.shape)  # (730, 82, 82)\n",
    "\n",
    "target_data5 = np.concatenate((target_data5, target_data55), axis = 0)\n",
    "target_data_04_05 = target_data5\n",
    "print(np.mean(target_data_04_05)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef418ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.366411\n"
     ]
    }
   ],
   "source": [
    "# 完整的2009/10的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date6 = np.datetime64('2009-01-01')\n",
    "end_date6 = np.datetime64('2010-12-31')\n",
    "start_index6 = np.where(dates == start_date6)[0][0]\n",
    "end_index6 = np.where(dates == end_date6)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data6 = data[start_index6:end_index6]\n",
    "target_data_09_10 = target_data6\n",
    "# print(target_data6.shape)  # (730, 82, 82)\n",
    "print(np.mean(target_data_09_10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0bbd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 82, 82)\n",
      "(306, 82, 82)\n",
      "24.298712\n"
     ]
    }
   ],
   "source": [
    "# 完整的2015/16的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date7 = np.datetime64('2015-01-01')\n",
    "end_date7 = np.datetime64('2016-02-28')\n",
    "start_index7 = np.where(dates == start_date7)[0][0]\n",
    "end_index7 = np.where(dates == end_date7)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data7 = data[start_index7:end_index7]\n",
    "\n",
    "print(target_data7.shape)  # (730, 82, 82)\n",
    "\n",
    "\n",
    "start_date77 = np.datetime64('2016-03-01')\n",
    "end_date77 = np.datetime64('2016-12-31')\n",
    "start_index77 = np.where(dates == start_date77)[0][0]\n",
    "end_index77 = np.where(dates == end_date77)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data77 = data[start_index77:end_index77]\n",
    "\n",
    "print(target_data77.shape)  # (730, 82, 82)\n",
    "\n",
    "target_data7 = np.concatenate((target_data7, target_data77), axis = 0)\n",
    "target_data_15_16 = target_data7\n",
    "# print(target_data7.shape)\n",
    "print(np.mean(target_data_15_16)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8134c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.22611\n"
     ]
    }
   ],
   "source": [
    "# 完整的2018/19的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date8 = np.datetime64('2018-01-01')\n",
    "end_date8 = np.datetime64('2019-12-31')\n",
    "start_index8 = np.where(dates == start_date8)[0][0]\n",
    "end_index8 = np.where(dates == end_date8)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data8 = data[start_index8:end_index8]\n",
    "target_data_18_19 = target_data8\n",
    "# print(target_data8.shape)  # (730, 82, 82)\n",
    "print(np.mean(target_data_18_19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f428fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到2年窗口期的Nino_with_MHW的平均数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "098a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(r'D:\\heat_wave\\atlantic\\EINO_NINA_plot\\Nino_data_with_MHW.npz', \n",
    "        target_Nino_data_82_83 = target_data_82_83, target_Nino_data_87_88 = target_data_87_88, \n",
    "        target_Nino_data_97_98 = target_data_97_98, target_Nino_data_02_03 = target_data_02_03,\n",
    "        target_Nino_data_04_05 = target_data_04_05, target_Nino_data_09_10 = target_data_09_10,\n",
    "        target_Nino_data_15_16 = target_data_15_16, target_Nino_data_18_19 = target_data_18_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdf57761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.106447"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_MHW = np.mean([target_data_82_83, target_data_87_88, target_data_97_98, target_data_02_03,\n",
    "                        target_data_04_05, target_data_09_10, target_data_15_16, target_data_18_19], axis = 0)\n",
    "\n",
    "data_with_MHW.shape\n",
    "np.mean(data_with_MHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b33553b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对这个数据，把MHW发生日期的数据设置为nan，进而平均时不考虑这个因素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e5d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 完整的1982/83的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without1 = np.datetime64('1982-01-01')\n",
    "end_date_without1 = np.datetime64('1983-12-31')\n",
    "start_index_without1 = np.where(dates == start_date_without1)[0][0]\n",
    "end_index_without1 = np.where(dates == end_date_without1)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without1 = data[start_index_without1:end_index_without1]\n",
    "# target_data_without_data_82_83 = target_data_without1\n",
    "# print(target_data_without_data_82_83.shape)  # (730, 82, 82)\n",
    "\n",
    "#去掉所有热浪\n",
    "\n",
    "#  整个1982-1983的数据\n",
    "start_date_nan = np.datetime64('1982-01-01') \n",
    "end_date_nan = np.datetime64('1983-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('1983-11-18')\n",
    "end_without_index_nan1 = np.datetime64('1983-11-28')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "target_data_without1[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('1983-12-15')\n",
    "end_without_index_nan2 = np.datetime64('1983-12-19')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without1[start_index_nan2 :end_index_nan2 ] = np.nan\n",
    "\n",
    "target_data_without_data_82_83 = target_data_without1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70011c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 82, 82)\n",
      "(306, 82, 82)\n",
      "(730, 82, 82)\n",
      "277\n"
     ]
    }
   ],
   "source": [
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without2 = np.datetime64('1987-01-01')\n",
    "end_date_without2 = np.datetime64('1988-02-28')\n",
    "start_index_without2 = np.where(dates == start_date_without2)[0][0]\n",
    "end_index_without2 = np.where(dates == end_date_without2)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without2 = data[start_index_without2:end_index_without2]\n",
    "\n",
    "print(target_data_without2.shape)  # (424, 82, 82)\n",
    "\n",
    "start_date_without22 = np.datetime64('1988-03-01')\n",
    "end_date_without22 = np.datetime64('1988-12-31')\n",
    "start_index_without22 = np.where(dates == start_date_without22)[0][0]\n",
    "end_index_without22 = np.where(dates == end_date_without22)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without22 = data[start_index_without22:end_index_without22]\n",
    "\n",
    "print(target_data_without22.shape)  # (306, 82, 82)\n",
    "\n",
    "target_data_without2 = np.concatenate((target_data_without2, target_data_without22), axis = 0)\n",
    "print(target_data_without2.shape)\n",
    "\n",
    "target_data_without2.shape\n",
    "\n",
    "# 整个1982-1983的数据\n",
    "start_date_nan = np.datetime64('1987-01-01') \n",
    "end_date_nan = np.datetime64('1988-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('1987-10-05')\n",
    "end_without_index_nan1 = np.datetime64('1987-11-12')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "print(start_index_nan1)\n",
    "# target_data1 = data[start_index_nan1 : end_index_nan1 + 1]\n",
    "# target_data.shape\n",
    "target_data_without2[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "\n",
    "target_data_without_data1 = target_data_without2\n",
    "# print(target_data_without_data1[260:280,:,:])\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('1987-11-21')\n",
    "end_without_index_nan2 = np.datetime64('1987-12-16')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "# target_data2 = data[start_index_nan2 : end_index_nan2 + 1]\n",
    "\n",
    "target_data_without_data1[start_index_nan2 :end_index_nan2] = np.nan\n",
    "target_data_without_data2 = target_data_without_data1\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan3 = np.datetime64('1988-02-10')\n",
    "end_without_index_nan3 = np.datetime64('1988-02-17')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan3 = np.where(dates_nan == start_without_index_nan3)[0][0]\n",
    "end_index_nan3 = np.where(dates_nan == end_without_index_nan3)[0][0]\n",
    "\n",
    "# target_data3 = data[start_index_nan3 : end_index_nan3 + 1]\n",
    "\n",
    "target_data_without_data2[start_index_nan3 :end_index_nan3] = np.nan\n",
    "\n",
    "target_data_without_data3 = target_data_without_data2\n",
    "target_data_without_data_87_88 = target_data_without_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11229147",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 82, 82)\n"
     ]
    }
   ],
   "source": [
    "# 完整的1997/98的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without3 = np.datetime64('1997-01-01')\n",
    "end_date_without3 = np.datetime64('1998-12-31')\n",
    "start_index_without3 = np.where(dates == start_date_without3)[0][0]\n",
    "end_index_without3 = np.where(dates == end_date_without3)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without3 = data[start_index_without3:end_index_without3]\n",
    "\n",
    "print(target_data_without3.shape)  # (730, 82, 82)\n",
    "\n",
    "\n",
    "\n",
    "# 整个1997-1998的数据\n",
    "start_date_nan = np.datetime64('1997-01-01') \n",
    "end_date_nan = np.datetime64('1998-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('1997-11-01')\n",
    "end_without_index_nan1 = np.datetime64('1997-11-06')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "# target_data1 = data[start_index_nan1 : end_index_nan1 + 1]\n",
    "# target_data.shape\n",
    "target_data_without3[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "target_data_without_data1 = target_data_without3\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('1998-02-25')\n",
    "end_without_index_nan2 = np.datetime64('1998-03-04')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without_data1[start_index_nan2 :end_index_nan2 ] = np.nan\n",
    "target_data_without_data2 = target_data_without_data1\n",
    "\n",
    "# target_data_without_data_97_98 = target_data_without_data2\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan3 = np.datetime64('1998-07-10')\n",
    "end_without_index_nan3 = np.datetime64('1998-07-25')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan3 = np.where(dates_nan == start_without_index_nan3)[0][0]\n",
    "end_index_nan3 = np.where(dates_nan == end_without_index_nan3)[0][0]\n",
    "\n",
    "target_data_without_data2[start_index_nan3 :end_index_nan3 ] = np.nan\n",
    "target_data_without_data3 = target_data_without_data2\n",
    "target_data_without_data_97_98 = target_data_without_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b608eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 82, 82)\n"
     ]
    }
   ],
   "source": [
    "# 完整的2002/03的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without4 = np.datetime64('2002-01-01')\n",
    "end_date_without4 = np.datetime64('2003-12-31')\n",
    "start_index_without4 = np.where(dates == start_date_without4)[0][0]\n",
    "end_index_without4 = np.where(dates == end_date_without4)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without4 = data[start_index_without4:end_index_without4]\n",
    "\n",
    "print(target_data_without4.shape)  # (730, 82, 82)\n",
    "\n",
    " #整个2002-2003的数据\n",
    "start_date_nan = np.datetime64('2002-01-01') \n",
    "end_date_nan = np.datetime64('2003-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('2002-07-11')\n",
    "end_without_index_nan1 = np.datetime64('2002-07-16')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "target_data_without4[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "target_data_without_data1 = target_data_without4\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('2002-09-05')\n",
    "end_without_index_nan2 = np.datetime64('2002-09-13')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without_data1[start_index_nan2 :end_index_nan2 ] = np.nan\n",
    "\n",
    "target_data_without_data2 = target_data_without_data1\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan3 = np.datetime64('2002-11-24')\n",
    "end_without_index_nan3 = np.datetime64('2002-12-09')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan3 = np.where(dates_nan == start_without_index_nan3)[0][0]\n",
    "end_index_nan3 = np.where(dates_nan == end_without_index_nan3)[0][0]\n",
    "\n",
    "target_data_without_data2[start_index_nan3 :end_index_nan3 ] = np.nan\n",
    "\n",
    "target_data_without_data3 = target_data_without_data2\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan4 = np.datetime64('2003-01-11')\n",
    "end_without_index_nan4 = np.datetime64('2003-01-19')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan4 = np.where(dates_nan == start_without_index_nan4)[0][0]\n",
    "end_index_nan4 = np.where(dates_nan == end_without_index_nan4)[0][0]\n",
    "\n",
    "target_data_without_data3[start_index_nan4 :end_index_nan4 ] = np.nan\n",
    "target_data_without_data4 = target_data_without_data3\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan5 = np.datetime64('2003-02-02')\n",
    "end_without_index_nan5 = np.datetime64('2003-03-16')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan5 = np.where(dates_nan == start_without_index_nan5)[0][0]\n",
    "end_index_nan5 = np.where(dates_nan == end_without_index_nan5)[0][0]\n",
    "\n",
    "target_data_without_data4[start_index_nan5 :end_index_nan5 ] = np.nan\n",
    "target_data_without_data5 = target_data_without_data4\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan6 = np.datetime64('2003-04-07')\n",
    "end_without_index_nan6 = np.datetime64('2003-05-10')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan6 = np.where(dates_nan == start_without_index_nan6)[0][0]\n",
    "end_index_nan6 = np.where(dates_nan == end_without_index_nan6)[0][0]\n",
    "\n",
    "target_data_without_data5[start_index_nan6 :end_index_nan6 ] = np.nan\n",
    "target_data_without_data6 = target_data_without_data5\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan7 = np.datetime64('2003-05-28')\n",
    "end_without_index_nan7 = np.datetime64('2003-06-24')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan7 = np.where(dates_nan == start_without_index_nan7)[0][0]\n",
    "end_index_nan7 = np.where(dates_nan == end_without_index_nan7)[0][0]\n",
    "\n",
    "target_data_without_data6[start_index_nan7 :end_index_nan7 ] = np.nan\n",
    "target_data_without_data7 = target_data_without_data6\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan8 = np.datetime64('2003-08-06')\n",
    "end_without_index_nan8 = np.datetime64('2003-09-01')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan8 = np.where(dates_nan == start_without_index_nan8)[0][0]\n",
    "end_index_nan8 = np.where(dates_nan == end_without_index_nan8)[0][0]\n",
    "\n",
    "target_data_without_data7[start_index_nan8 :end_index_nan8 ] = np.nan\n",
    "target_data_without_data8 = target_data_without_data7\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan9 = np.datetime64('2003-10-13')\n",
    "end_without_index_nan9= np.datetime64('2003-11-06')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan9 = np.where(dates_nan == start_without_index_nan9)[0][0]\n",
    "end_index_nan9 = np.where(dates_nan == end_without_index_nan9)[0][0]\n",
    "\n",
    "target_data_without_data8[start_index_nan9 :end_index_nan9 ] = np.nan\n",
    "target_data_without_data9 = target_data_without_data8\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan10 = np.datetime64('2003-12-11')\n",
    "end_without_index_nan10= np.datetime64('2003-12-15')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan10 = np.where(dates_nan == start_without_index_nan10)[0][0]\n",
    "end_index_nan10 = np.where(dates_nan == end_without_index_nan10)[0][0]\n",
    "\n",
    "target_data_without_data9[start_index_nan10 :end_index_nan10 ] = np.nan\n",
    "target_data_without_data10 = target_data_without_data9\n",
    "\n",
    "target_data_without_data_02_03 = target_data_without_data10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1cbd51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 82, 82)\n",
      "(671, 82, 82)\n",
      "(730, 82, 82)\n"
     ]
    }
   ],
   "source": [
    "# 完整的2004/05的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without5 = np.datetime64('2004-01-01')\n",
    "end_date_without5 = np.datetime64('2004-02-28')\n",
    "start_index_without5 = np.where(dates == start_date_without5)[0][0]\n",
    "end_index_without5 = np.where(dates == end_date_without5)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without5 = data[start_index_without5:end_index_without5]\n",
    "\n",
    "print(target_data_without5.shape)  # (730, 82, 82)\n",
    "\n",
    "start_date_without55 = np.datetime64('2004-03-01')\n",
    "end_date_without55 = np.datetime64('2005-12-31')\n",
    "start_index_without55 = np.where(dates == start_date_without55)[0][0]\n",
    "end_index_without55 = np.where(dates == end_date_without55)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without55 = data[start_index_without55:end_index_without55]\n",
    "\n",
    "print(target_data_without55.shape)  # (730, 82, 82)\n",
    "\n",
    "target_data_without5 = np.concatenate((target_data_without5, target_data_without55), axis = 0)\n",
    "print(target_data_without5.shape)\n",
    "\n",
    "#整个2004-2005的数据\n",
    "start_date_nan = np.datetime64('2004-01-01') \n",
    "end_date_nan = np.datetime64('2005-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('2004-07-28')\n",
    "end_without_index_nan1 = np.datetime64('2004-08-04')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "target_data_without5[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "target_data_without_data1 = target_data_without5\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('2004-08-24')\n",
    "end_without_index_nan2 = np.datetime64('2004-09-02')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without_data1[start_index_nan2 :end_index_nan2 ] = np.nan\n",
    "target_data_without_data2 = target_data_without_data1\n",
    "# target_data_without_data2\n",
    "\n",
    "target_data_without_data_04_05 =  target_data_without_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "527afae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的2009/10的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without6 = np.datetime64('2009-01-01')\n",
    "end_date_without6 = np.datetime64('2010-12-31')\n",
    "start_index_without6 = np.where(dates == start_date_without6)[0][0]\n",
    "end_index_without6 = np.where(dates == end_date_without6)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without6 = data[start_index_without6:end_index_without6]\n",
    "target_data_without_data_6 = target_data_without6\n",
    "# print(target_data_without_data_09_10.shape)  # (730, 82, 82)\n",
    "\n",
    "#整个2009-2010的数据\n",
    "start_date_nan = np.datetime64('2009-01-01') \n",
    "end_date_nan = np.datetime64('2010-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('2009-09-20')\n",
    "end_without_index_nan1 = np.datetime64('2010-04-18')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "target_data_without_data_6[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "\n",
    "target_data_without_data_7 = target_data_without_data_6\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('2010-05-06')\n",
    "end_without_index_nan2 = np.datetime64('2010-06-28')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without_data_7[start_index_nan7 :end_index_nan7 ] = np.nan\n",
    "\n",
    "target_data_without_data_09_10 = target_data_without_data_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e1ff80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 82, 82)\n",
      "(306, 82, 82)\n",
      "(730, 82, 82)\n"
     ]
    }
   ],
   "source": [
    "# 完整的2015/16的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without7 = np.datetime64('2015-01-01')\n",
    "end_date_without7 = np.datetime64('2016-02-28')\n",
    "start_index_without7 = np.where(dates == start_date_without7)[0][0]\n",
    "end_index_without7 = np.where(dates == end_date_without7)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without7 = data[start_index_without7:end_index_without7]\n",
    "\n",
    "print(target_data_without7.shape)  # (730, 82, 82)\n",
    "\n",
    "\n",
    "start_date_without77 = np.datetime64('2016-03-01')\n",
    "end_date_without77 = np.datetime64('2016-12-31')\n",
    "start_index_without77 = np.where(dates == start_date_without77)[0][0]\n",
    "end_index_without77 = np.where(dates == end_date_without77)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without77 = data[start_index_without77:end_index_without77]\n",
    "\n",
    "print(target_data_without77.shape)  # (730, 82, 82)\n",
    "\n",
    "target_data_without7 = np.concatenate((target_data_without7, target_data_without77), axis = 0)\n",
    "print(target_data_without7.shape)\n",
    "\n",
    "# 整个2015-2016的数据\n",
    "start_date_nan = np.datetime64('2015-01-01') \n",
    "end_date_nan = np.datetime64('2016-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('2015-01-12')\n",
    "end_without_index_nan1 = np.datetime64('2015-02-17')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "target_data_without7[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "target_data_without_data1 = target_data_without7\n",
    "target_data_without_data1\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('2015-03-24')\n",
    "end_without_index_nan2 = np.datetime64('2015-03-31')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without_data1[start_index_nan2 :end_index_nan2 ] = np.nan\n",
    "target_data_without_data2 = target_data_without_data1\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan3 = np.datetime64('2015-12-14')\n",
    "end_without_index_nan3 = np.datetime64('2015-12-23')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan3 = np.where(dates_nan == start_without_index_nan3)[0][0]\n",
    "end_index_nan3 = np.where(dates_nan == end_without_index_nan3)[0][0]\n",
    "\n",
    "target_data_without_data2[start_index_nan3 :end_index_nan3 ] = np.nan\n",
    "target_data_without_data3 = target_data_without_data2\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan4 = np.datetime64('2016-02-29')\n",
    "end_without_index_nan4 = np.datetime64('2016-03-05')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan4 = np.where(dates_nan == start_without_index_nan4)[0][0]\n",
    "end_index_nan4 = np.where(dates_nan == end_without_index_nan4)[0][0]\n",
    "\n",
    "target_data_without_data3[start_index_nan4 :end_index_nan4 ] = np.nan\n",
    "target_data_without_data4 = target_data_without_data3\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan5 = np.datetime64('2016-06-11')\n",
    "end_without_index_nan5 = np.datetime64('2016-10-03')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan5 = np.where(dates_nan == start_without_index_nan5)[0][0]\n",
    "end_index_nan5 = np.where(dates_nan == end_without_index_nan5)[0][0]\n",
    "\n",
    "target_data_without_data4[start_index_nan5 :end_index_nan5 ] = np.nan\n",
    "target_data_without_data5 = target_data_without_data4\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan6 = np.datetime64('2016-10-07')\n",
    "end_without_index_nan6 = np.datetime64('2016-10-12')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan6 = np.where(dates_nan == start_without_index_nan6)[0][0]\n",
    "end_index_nan6 = np.where(dates_nan == end_without_index_nan6)[0][0]\n",
    "\n",
    "target_data_without_data5[start_index_nan6 :end_index_nan6 ] = np.nan\n",
    "target_data_without_data6 = target_data_without_data5\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan7 = np.datetime64('2016-12-09')\n",
    "end_without_index_nan7 = np.datetime64('2016-12-15')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan7 = np.where(dates_nan == start_without_index_nan7)[0][0]\n",
    "end_index_nan7 = np.where(dates_nan == end_without_index_nan7)[0][0]\n",
    "\n",
    "target_data_without_data6[start_index_nan7 :end_index_nan7 ] = np.nan\n",
    "target_data_without_data7 = target_data_without_data6\n",
    "\n",
    "target_data_without_data_15_16 = target_data_without_data7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc88b6ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730, 82, 82)\n"
     ]
    }
   ],
   "source": [
    "# 完整的2018/19的数据\n",
    "start_date = np.datetime64('1982-01-01')\n",
    "dates = start_date + np.arange(14610) * np.timedelta64(1, 'D')\n",
    "\n",
    "# 选择特定日期范围的数据\n",
    "start_date_without8 = np.datetime64('2018-01-01')\n",
    "end_date_without8 = np.datetime64('2019-12-31')\n",
    "start_index_without8 = np.where(dates == start_date_without8)[0][0]\n",
    "end_index_without8 = np.where(dates == end_date_without8)[0][0] + 1  # 注意要加 1，因为切片时 end_index 是开区间\n",
    "target_data_without8 = data[start_index_without8:end_index_without8]\n",
    "\n",
    "print(target_data_without8.shape)  # (730, 82, 82)\n",
    "\n",
    "\n",
    "# 整个2018-2019的数据\n",
    "start_date_nan = np.datetime64('2018-01-01') \n",
    "end_date_nan = np.datetime64('2019-12-31') \n",
    "dates_nan = np.arange(start_date_nan, end_date_nan + np.timedelta64(1,'D'), dtype = 'datetime64[D]')\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan1 = np.datetime64('2019-01-13')\n",
    "end_without_index_nan1 = np.datetime64('2019-02-04')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan1 = np.where(dates_nan == start_without_index_nan1)[0][0]\n",
    "end_index_nan1 = np.where(dates_nan == end_without_index_nan1)[0][0]\n",
    "\n",
    "target_data_without8[start_index_nan1 :end_index_nan1 ] = np.nan\n",
    "target_data_without_data1 = target_data_without8\n",
    "# target_data_without_data1\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan2 = np.datetime64('2019-02-25')\n",
    "end_without_index_nan2 = np.datetime64('2019-06-28')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan2 = np.where(dates_nan == start_without_index_nan2)[0][0]\n",
    "end_index_nan2 = np.where(dates_nan == end_without_index_nan2)[0][0]\n",
    "\n",
    "target_data_without_data1[start_index_nan2 :end_index_nan2 ] = np.nan\n",
    "target_data_without_data2 = target_data_without_data1\n",
    "\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan3 = np.datetime64('2019-07-11')\n",
    "end_without_index_nan3 = np.datetime64('2019-07-20')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan3 = np.where(dates_nan == start_without_index_nan3)[0][0]\n",
    "end_index_nan3 = np.where(dates_nan == end_without_index_nan3)[0][0]\n",
    "\n",
    "target_data_without_data2[start_index_nan3 :end_index_nan3 ] = np.nan\n",
    "target_data_without_data3 = target_data_without_data2\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan4 = np.datetime64('2019-07-24')\n",
    "end_without_index_nan4 = np.datetime64('2019-07-28')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan4 = np.where(dates_nan == start_without_index_nan4)[0][0]\n",
    "end_index_nan4 = np.where(dates_nan == end_without_index_nan4)[0][0]\n",
    "\n",
    "target_data_without_data3[start_index_nan4 :end_index_nan4 ] = np.nan\n",
    "target_data_without_data4 = target_data_without_data3\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan5 = np.datetime64('2019-08-01')\n",
    "end_without_index_nan5 = np.datetime64('2019-08-08')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan5 = np.where(dates_nan == start_without_index_nan5)[0][0]\n",
    "end_index_nan5 = np.where(dates_nan == end_without_index_nan5)[0][0]\n",
    "\n",
    "target_data_without_data4[start_index_nan5 :end_index_nan5 ] = np.nan\n",
    "target_data_without_data5 = target_data_without_data4\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan6 = np.datetime64('2019-09-12')\n",
    "end_without_index_nan6 = np.datetime64('2019-09-16')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan6 = np.where(dates_nan == start_without_index_nan6)[0][0]\n",
    "end_index_nan6 = np.where(dates_nan == end_without_index_nan6)[0][0]\n",
    "\n",
    "target_data_without_data5[start_index_nan6 :end_index_nan6 ] = np.nan\n",
    "target_data_without_data6 = target_data_without_data5\n",
    "\n",
    "#将热浪事件设置为nan的时间\n",
    "start_without_index_nan7 = np.datetime64('2019-11-17')\n",
    "end_without_index_nan7 = np.datetime64('2019-11-23')\n",
    "\n",
    "#获取对应日期的索引\n",
    "start_index_nan7 = np.where(dates_nan == start_without_index_nan7)[0][0]\n",
    "end_index_nan7 = np.where(dates_nan == end_without_index_nan7)[0][0]\n",
    "\n",
    "target_data_without_data6[start_index_nan7 :end_index_nan7 ] = np.nan\n",
    "target_data_without_data7 = target_data_without_data6\n",
    "\n",
    "target_data_without_data_18_19 = target_data_without_data7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "248e7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  没有去除热浪的数据\n",
    "#target_data1 target_data2 target_data3 target_data4 target_data5 target_data6 target_data7 target_data8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a36c805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(r'D:\\heat_wave\\atlantic\\EINO_NINA_plot\\Nino_data_without_MHW1.npz', \n",
    "        target_Nino_data_without_data_82_83 = target_data_without_data_82_83,\n",
    "        target_Nino_data_without_data_87_88 = target_data_without_data_87_88,\n",
    "        target_Nino_data_without_data_97_98 = target_data_without_data_97_98,\n",
    "        target_Nino_data_without_data_02_03 = target_data_without_data_02_03,\n",
    "        target_Nino_data_without_data_04_05 = target_data_without_data_04_05,\n",
    "        target_Nino_data_without_data_09_10 = target_data_without_data_09_10,\n",
    "        target_Nino_data_without_data_15_16 = target_data_without_data_15_16,\n",
    "        target_Nino_data_without_data_18_19 = target_data_without_data_18_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fac9660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 82, 82)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_MHW = np.nanmean([target_data_without_data_82_83, target_data_without_data_87_88,\n",
    "                              target_data_without_data_97_98, target_data_without_data_02_03, \n",
    "                              target_data_without_data_04_05, target_data_without_data_09_10,\n",
    "                              target_data_without_data_15_16, target_data_without_data_18_19], axis = 0)\n",
    "data_without_MHW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8af36b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_with_MHW:21.7932071685791\n"
     ]
    }
   ],
   "source": [
    "print('data_with_MHW:{}'.format(np.min(data_with_MHW)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "771eb001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_data_without_MHW:17.31571388244629\n"
     ]
    }
   ],
   "source": [
    "print('min_data_without_MHW:{}'.format(np.min(data_without_MHW)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "688dd1f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_51544/301302046.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_with_MHW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_with_MHW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3472\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3474\u001b[1;33m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[0;32m   3475\u001b[0m                           out=out, **kwargs)\n\u001b[0;32m   3476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mrcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean of empty slice.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "data_with_MHW = np.mean(data_with_MHW, axis = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7faa29c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.106453"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_MHW.shape\n",
    "np.mean(data_with_MHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3702e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_MHW = np.mean(data_without_MHW, axis = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0d929fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.07522"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without_MHW.shape\n",
    "np.mean(data_without_MHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80b2719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.98290794396158"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clim.shape\n",
    "np.mean(data_clim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b81cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056f954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfdda968",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (730,82,82) (730,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_51544/3825161859.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_with_MHW\u001b[0m \u001b[1;33m-\u001b[0m  \u001b[0mdata_clim\u001b[0m\u001b[1;31m# np.array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdata_without_MHW\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata_clim\u001b[0m\u001b[1;31m# pd.Series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Jan[0]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Apr[0]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Jul[0]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Oct[0]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Jan[1]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Apr[1]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Jul[1]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Oct[1]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (730,82,82) (730,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y1=data_with_MHW -  data_clim# np.array\n",
    "y2= data_without_MHW - data_clim# pd.Series\n",
    "\n",
    "labels = ['Jan[0]','Apr[0]','Jul[0]','Oct[0]','Jan[1]','Apr[1]','Jul[1]','Oct[1]','']\n",
    "\n",
    "#设置横坐标位置\n",
    "plt.figure(figsize = (8,4), dpi = 300)\n",
    "x_positions = np.linspace(0, len(y1)-1, 9)\n",
    "\n",
    "\n",
    "plt.plot(y1,color = '#D62728', label='With MHWs')\n",
    "plt.plot(y2,color = '#1F77B4', label='Without MHWs')  # x可省略,默认[0,1..,N-1]递增\n",
    "plt.xticks(x_positions, labels)\n",
    "plt.legend(frameon=False, loc = 'upper left')\n",
    "# plt.xlabel('Time')\n",
    "plt.xlim(0,730)\n",
    "plt.ylabel('South Atlantic SSTA (℃)')\n",
    "\n",
    "#添加竖线\n",
    "plt.axvline(x=243,color = '#7F7F7F', linestyle = '-')\n",
    "plt.axvline(x=396,color = '#7F7F7F', linestyle = '-')\n",
    "\n",
    "#添加文字\n",
    "plt.text(263,-0.35,'EI Nino-P1',fontsize = 14, color = '#7F7F7F')\n",
    "\n",
    "\n",
    "#添加竖线\n",
    "plt.axvline(x=485,color = '#7F7F7F', linestyle = '-')\n",
    "plt.axvline(x=608,color = '#7F7F7F', linestyle = '-')\n",
    "\n",
    "#添加文字\n",
    "plt.text(490,-0.35,'EI Nino-P2',fontsize = 14, color = '#7F7F7F')\n",
    "\n",
    "plt.show() # plt.show()前可加多个plt.plot(),画在同一张图上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af0ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385cc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36c45d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start_date = pd.to_datetime('1993-01-01') # 假设起始时间为2022年1月1日\n",
    "index_9_1 = (pd.to_datetime('1994-09-01') - start_date).days\n",
    "index_9_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046aeed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f51b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13862ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4a630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "660e0631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e19886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
